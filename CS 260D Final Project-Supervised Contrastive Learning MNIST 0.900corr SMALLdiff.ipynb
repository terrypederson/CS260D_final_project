{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16543,"status":"ok","timestamp":1702409869293,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"vX3SJPVGyu-i","outputId":"29695b8e-2c7f-4698-df05-f0babc2ac4c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17941,"status":"ok","timestamp":1702409887221,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"Si1A9ZNLICCF","outputId":"82033946-847a-445a-a729-e92a63c00979"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.2/126.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["\n","%pip install -q spuco"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16486,"status":"ok","timestamp":1702409903698,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"SD4tBgEQiOBG","outputId":"cbcb8e5c-bd3d-4e1c-eeab-ee1f0c5cf721"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m92.2/115.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install -q pytorch-metric-learning"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FO39jlv-IQ7B","executionInfo":{"status":"ok","timestamp":1702409920497,"user_tz":480,"elapsed":16814,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","from torch.utils.data import DataLoader, Dataset\n","from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n","import torchvision.transforms as transforms\n","import cv2\n","import torchvision.models as models\n","from pytorch_metric_learning import losses\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import os\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"EiErEch7gn0V","executionInfo":{"status":"ok","timestamp":1702409920498,"user_tz":480,"elapsed":5,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":["class Config:\n","  spurious_correlation_strength  = 0.9\n","  spurious_feature_difficulty = SpuriousFeatureDifficulty.MAGNITUDE_SMALL\n","  label_noise = 0.001\n","  size = 28\n","  flip_probability = 0.5\n","  batch_size = 256\n","  backbone_pretrained = False\n","  projection_dim = 128\n","  T_max = 10\n","  lr = 5e-4\n","  min_lr = 1e-5\n","  weight_decay = 1e-6\n","  temperature = 0.5"]},{"cell_type":"markdown","metadata":{"id":"6g19rJPYOJ6N"},"source":["## Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60563,"status":"ok","timestamp":1702409981057,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"hB4joWrQQuST","outputId":"3cb07de2-0315-4570-f1f0-93a481110fdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 78563175.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /data/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 18713995.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /data/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 26897419.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /data/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 15228240.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /data/mnist/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 48004/48004 [00:17<00:00, 2777.00it/s]\n","100%|██████████| 10000/10000 [00:01<00:00, 6210.06it/s]\n"]}],"source":["classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n","difficulty = SpuriousFeatureDifficulty.MAGNITUDE_SMALL\n","\n","trainset = SpuCoMNIST(\n","    root=\"/data/mnist/\",\n","    spurious_feature_difficulty=Config.spurious_feature_difficulty,\n","    spurious_correlation_strength=Config.spurious_correlation_strength,\n","    classes=classes,\n","    split=\"train\",\n","    label_noise=Config.label_noise\n",")\n","trainset.initialize()\n","\n","valset = SpuCoMNIST(\n","    root=\"/data/mnist/\",\n","\n","    spurious_feature_difficulty=Config.spurious_feature_difficulty,\n","    spurious_correlation_strength=Config.spurious_correlation_strength,\n","    classes=classes,\n","    split=\"test\",\n","    label_noise=Config.label_noise\n",")\n","valset.initialize()\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1702409981058,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"a8MxdHy-fVyu","outputId":"44f5ffff-20be-4697-bf18-c69aae115c3c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n","  warnings.warn(\n"]}],"source":["\n","## Augmentation\n","from torchvision.transforms import v2\n","\n","size = 28\n","train_transform = transforms.Compose([\n","    v2.ToImage(),\n","    v2.RandomResizedCrop(size),\n","    v2.RandomHorizontalFlip(p=0.5),\n","    v2.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n","    v2.RandomGrayscale(p=0.2),\n","    v2.ToTensor(),\n","    v2.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    v2.ToImage(),\n","    v2.ToTensor(),\n","    v2.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"GCkBS9IDOMIE","executionInfo":{"status":"ok","timestamp":1702409981058,"user_tz":480,"elapsed":7,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":["class SpucoDataset(Dataset):\n","    def __init__(self, dataset, transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, index):\n","        img = self.dataset[index][0]\n","        label = self.dataset[index][1]\n","\n","        if self.transform is None:\n","          return img, label\n","\n","        augmented_img = self.transform(img)\n","        return (img, augmented_img), label"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"XYevgaDhSRkb","executionInfo":{"status":"ok","timestamp":1702409981058,"user_tz":480,"elapsed":6,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":["train_dataset = SpucoDataset(trainset, train_transform)\n","val_dataset = SpucoDataset(valset, test_transform)\n","\n","dataloaders = {'train': DataLoader(dataset=train_dataset, batch_size=Config.batch_size, num_workers=2, pin_memory=True, shuffle=True),\n","         'eval': DataLoader(dataset=val_dataset, batch_size=Config.batch_size, num_workers=2, pin_memory=True, shuffle=False)}"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1702409981353,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"MtYsAtuILi8i","outputId":"f5068d5a-4844-4175-a8d3-71cfcf452232"},"outputs":[{"output_type":"stream","name":"stdout","text":["Group: (3, 3), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABnElEQVR4nO3UvY4BURQH8GOtQqIxQqIQEolCI5OQ+BqRKEU1XmEqOmqdxgN4Ax5B4xGMYgoEMyKIEAmC4grObCG7hZnE7LpbbLL/7p6c+8st7jkAZRXKKlDNG13uP38qJm0pn88LgrBcLgkhjUZjtVrJsvwqOplMfD7f1/F4PPZ6Pd3Li8WiVquJovhQf9e2CoIQCoX6/X4wGGRZNp1OR6PR+Xzu8XjuDdfrdbPZuN1uAJjNZlpU56UPsdvtLMuKohiJRO4VQshoNBoMBgzDFIvFer3+FDEUnudvt5skSQzD0BFdLtd6vVZVled53YafzH6hUHA6nbvdbjgcvva8zyQSifP5jIipVIqOCADVahUR2+22xWKhI1qt1m63SwiJx+N0RACoVCqI2Gq1qInZbPZyuez3+1gsRkd0OByKoiBis9mkI5rN5k6ng4jj8djv99NBA4EAIiJiLpejI3q93ul0ioilUslker4rDOX+MRExHA7TETmOOxwO30WfzH4ymbTZbACgKMrpdDKI6ixpbSRJymQy2+3WIPor+QBlbLh4lfl9vwAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (1, 1), Qty: 409\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABt0lEQVR4nO2VPWsCQRCGJyF3oAjiWcg1KlgKVteLvR+N4O8Q/IDrtVJbOy2sDlEOewtB7ixE4oIIVnandjanuI5JsSCSEA2XDaTIW+3sDs/MsDO78PoGr2/AV8+ceb+np/vHhULB5XLFYrFsNgsAzWbTNM1Op+M8oKZp+Emr1SoYDHIgLhaLRqOh6zozVVV1QlQU5XQ6ISIhJBwOezweABBFcTabIWKtVnMCTaVSlFJCiCzL101VVY/HIyImEgknUAAIhUKSJN3uzOdzVr5z6AcVi8XD4YCIhmG43W4OxGQyyYiWZcXj8Yf+35ooRVFEUQQATdNGo9FPcwQAXddt20bEdrvN2uCnkmV5t9sh4na7jUQiHIgAYBgGu/F6vc6HmE6nWWMOh0M+hfv9/slkwjnNarXKiL1ej0+aAMAKR8TbSf2mXh56SJJEKb2a+/2eUioIgtfrBQCfz5fP59kRIpbLZdu2H0MJIbdmt9u1LCsQCORyuc/Om82mUql8+fL3+/1MJnM/3vl8vlwuADAYDKbTKQCMx2PTNO99J6VSSRAEto5Go9fUWq3Wer1mgZfL5f3A//rDegerHP0+R1wtXgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (0, 1), Qty: 423\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABIklEQVR4nO2QMYqDQBSGHxsCYdJZqJ1WWg1imSIEYucpvIDnEIS0uYMwhacIIgyky3Q2CQhBJGBh43OLLYU4Wada9qv/+fjewHWE6whq+VLs++s4joOIcRy/2Xz8p77vI+Lj8VgQNiFJktfrpdJIKe267nw+v599dr7ruoSQLMsWhE0oy7Kqqu12q8xo2zYiCiFmlx+cfzgcAOD5fKqUUkoBIE1T+Scz7Ha7pmk455vNZnYsWxoEgaZpQoi+75VJPc8bx5ExJrmfxzTNuq5vt5vkXqo0iiJd14uiUCm1LAsA2raVlEpxv98R8Xg8Su7nS/f7vWEYy6omnE4nROScr1YrySczpYSQMAwBgDE2DMPSwB/W6/XlcsnznBCixvjP7/gGCQJitl18iqsAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (0, 4), Qty: 423\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABoklEQVR4nO2UMYvCMBTHazgXXe1qhCJ+AKGdTEfbQfBriCj0G9RREBEKzn4BHYuDLrqom5MorYNIBFfHEHpDuVxpr9dGOh33317e6y8vyftXWAreUvCETAWyxf1JFYvF6XRKKT0cDhDCbKDVapUQQgihlHa73cT65IcSRXE2m3E18fF7ut/vt9ttWZbZCkIIAHA8HjebDddO36KUkoBY6DhOvV5/h2jbtud5NKDn83m9XlnITVRV1XXdYKeWZbVaLYSQaZr+SqfT4SBWKpXH48HO6zjOcDgsFAp+FkKIMSaEvF4vwzDy+XwqaHCGVqtVqVQKFfR6PbalJEl80P1+Xy6XowUQwt1uFweNnVMAAABAUZTb7RbN5nI58KXBYJCq09Fo5HcaV/DO8c/ncxxUFEVVVdkzYox/vB8+6GQyYUPmum6j0YjWJNg0JNu2a7UaC0+n03a7Tfvx5XLxPaPruq7r9/vdD0MG42pIMAwjZPZoaFkWH5R5JgrFGK/Xa0mSmME4hBAaj8dRaJqfdII0TVssFoSQ+XzebDY1TUs7QP/KVp+gV4MUkmIMdwAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (2, 0), Qty: 375\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABtElEQVR4nO2VsYryQBDHJ0ckRgUhoI2IQiqDKRQsLLVQsLDRzjfQwkewsxF9BbGLYieWgg+QRrSxELRyi1SCRHSX/YrAx3G3G93D7u5fBHb+M7/MZrIsUAAKb9bHu4F/eqZ8Pn88HplWpVJJJpNM68mgqtWqoihMq16vDwYDYagsy7Vajefatm0YRjgcFoOWSqVisTgej5mupmmGYYRCIR/CV5mm6TjOfr+PRCLMhPV6jTGOxWICUMuyXNctFAq8NimlhBABaLPZvFwu2+2WlzAcDgkhq9UqEAi8Cp1OpxjjdrvNdNPpNELofr+Xy+VXidFo9HQ6YYx5Cf1+H2Pssw/G9BVFSSQSlmXxanRdB4DdbvdqmwCgqqpt25vNRtO07248HieEEEI6nQ6PIH8Pua57OBwajcZyuRyNRv/j2WxW1/VUKkUpBQDvKaBMJjObza7XK/4khND5fH48Ht5SVVVeueSDzuVy3ufzNJ/PAWAymbRaLQCQZcYuf6her+d1apomL0f4bZIkSZIEAD6/lDCUUvp0RMIXXzAYBIDb7SZa6CeEkOM43W73ndDFYiFw5H+l/gFEFLYnkl9xyQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (2, 2), Qty: 375\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABwUlEQVR4nO2Vv+tpYRzHP90uJZ2kOCYSmaTEYhELKRlNDAaDf8CuTlmMBlFOOBkUi1H5MZkQMzZ11Ek5ZVKfz7nDqW93uOeg75m+3df2PM+7V+96nk8PgFIEpQiG8stY3X9ewLLsarWq1+ter1crY7PZcrmcyWR6y2i32yVJej6fo9FIx3g6nWRZDgQCr40Oh2M+nyNis9nUiTUaDUQsl8tv1Uyn04iIiE6nUysTDAaJaDKZMAzz2siybLvdRsRSqaRjFEWRiIrF9wZHEARFUTabjdVq1cpUKhUi4nn+LSMADAYDRJxOp/+8U4vFwnHc7XZDRC3Db62DbDY7m83u93ur1fraTCQSyWQyFosBwHg8frcmAESj0cvlol4UEeFffC2Px6Pf7/+g6Xa7DYVC4XA4k8lUq1VJkvr9vnokCMLhcACA9Xp9Pp8/aKqDz+cjot1up/PUPqbX6yFiKpUyzJjP54lIluVIJGKYlOd5IhoOh4YZAUAUxcfjYWRNdYSu16thRgDY7/eI2O12AYBhGI/HoxP+7ONDxEKhsFwuOY77VkcVtak6UZ1Ox+12GyCNx+OLxaJWq7lcLrPZbIDx5/IHYR7lGUucf3MAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (4, 2), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABrUlEQVR4nO3TLcsqURDA8UFQd0E2rAgG0aCIGMTgR7AJmhTFJ/kVthuNvgTBtGDwJMG8tm3ajOKGFYtBXRRWEWXmPEGQG65P2Hu4t9x/Okz4hTkMAP8C/gVC84nl/vfvUhRlOByapun3+8WIzWZzu90iIiKGw2EBYiwWOxwORPRCGWOqqv4p2u/3EfGNIqLjOJqmBQIBj2IikbhcLoi4Wq0Mw3i7+/0+Go16RCuVChGZpgkAkiS1Wi3LsoiIc75cLn/ew8czDQaDnPNerwcA9/td13XLsjjnnPPb7fZ4PLygjUYDAEql0ntSKBRej8Vi4bruD+jHarXaa6GZTKZarTLGns/n6XQiouPxmM1mvaCqqjqO8+vvG4aRSqXW6zUijkYjLygAFIvF8/nMOSeiwWAgSRIAdDodIrJtO5lMend1Xe92u6FQ6DWRZXk2myHieDz2iP62er2OiLvdTsCBvfP5fIwxRGy328JQAMjn89frFRHT6bRIV9M0IppOp7IsC0MjkchmsyGiXC4nDAWAeDxORJPJRCQKAPP53HVdjwf2KUVRbNsul8si0b/XN9Nj+LHWbBetAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (2, 4), Qty: 374\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB1klEQVR4nO2Uz+spURTA7+OlGCUlNtOkLK2Qja2NUv4C/gbsLBQLVsqKGnvFblIWkqUyNiY1wuRHSvmxU2ZkzJl5i+lJvs+L+c7m1fusTqd7P51zO+eiDlI6SEG6YtBX96/xQ8Mdu91OEIQabzabdDrNsizHcePxWEsJ0Wi0VqvN53P4zXQ6FQRBjT9zeTyecrl8Pp8lSYLX3M//fEeK43gymfyan81mk8nka/6l1OFwpFKpfr/f6XREUTydTjzPYxjW7XZZlh0OhwzDXC4XnuffKQshhDAMG41GABCLxdSM2+1GCBEEYTBoGm2TydRqtQCgUChYLBYtiiesVmuxWASAw+Fgs9l0MCKE4vE4AKzXaxzHNUueHygUCiGEGIbZbrffqu6R4/EIAIIg5HI5n8+nTfK8poqiyLKsxrIskyRJ0zRBEIvFQh1Jr9c7GAw+66NUKv1lZ1T2+32z2fxAajQag8Egx3Gr1ep2u73ySpKUzWY/8N4Jh8ORSISm6T96KYrSIlXJZDIAcL1eSZIMBAL1el0Hqd/vv1fX6/XuH1WlUtEuNZvNjUbjsXFRFCmKwjBMuxQh5HK52u32brcDgOVymc/nv6V7JJFIVKtVp9Opm/E/7/MLuJVn1fb4ozgAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (0, 3), Qty: 423\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB9klEQVR4nO3UP+g5YRwH8M/v/ExfiZhkOoOkLAa36GR0l1KKwU7Z9B1kUspuwsBqspCiZMFdyEL+LBYWOcudbtF5fAf165vuuK+f5Ve/9/T0eT7P6/kMTw/A5xU+r/DWYO/l/rX8etrh9XoZhrHb7TRNUxTVarUAgGXZfr//yoV6vb7ZbIqieDweBUFA3yKKIsdx4XD4x5MWi8V4PH5br1YrjuMEQQAADMMCgQAAnE4nr9c7m83Ujul0OjmOQwhtt1ufz2e1WnU63W0Lw7BsNitJ0vV6rdfrRqNRLUoQBELocrkkk0nZhnw+fz6fEUIURalFSZJECFWr1Qc9m80GIVSpVO7qio8/l8sBwGg0eoB2Oh0AIAhCFYrjuMVi4Xl+Pp8/QHu9nmxdHo3FYjiOd7tdhmEeoEqRR6PRKM/zhULhBVERBYD1ej0YDN6Gfnx8aLXa1zhFNBKJ2Gw2NYeDwSAASJL0HFUZt9tN0zQAZDKZ96ButzuVShkMhuFweHutT+L3+3meZ1lWqUGj0dRqNYTQbrfzeDxqB1kul4vFwmw239VdLlepVBqPx7cPkCRJ2eO/lVyHw9Fut/f7/feix+MxmUwAcDweG43GZDJROyYAhEKh6XSK5CJJ0uFwSKfTP+D+xGKxzGazO7FcLicSiVe4//nLfAF9sv3GS/w5gQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (4, 4), Qty: 396\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABuklEQVR4nO2VvcriQBSGD8umCCIIMoU2BtTGYOFUdqYQC/USvATL3INiKViJIGJhwEotYhG9gBSpjBegGOwGiyAyh/mKgIiL+yW7qZZ9quEw5+FlfsEEYYKAWPkRr+4//x6VSmWz2SCiEAIRDcMoFAqJRKLdbsuyHFknSVK9Xj+fz5xzzjkiBoPpdGpZFiJ2Op1fu37+XkopNU0TADzP63a7vu8DQC6X831/OBw+Hg/P86LFVFX1er1yzrfbLaX0Wc9ms7Ztc857vV40IwAsFgtEXK1WxWLxtd5oNIJFqNVq0Yzj8RgRb7dbqVR6rUuStN/vEXG320WO6TgO5/xyubwZ+/1+EFPTtBikiqIMBoPAeDqdUqnUp96Pu++6brlcTqfTjuMEFUJIJpMRQgCAZVmMschJZVler9fPgxnQarUMw+CcV6vVyMYnmqbpuq7rerPZBIDRaISIx+OREPLn0jeC4LPZLDajoiiIyBh7vQh/y2QyQcT5fB6bUVVVxtinF+SNsB8fpTSZTAoh7vd7bFJCiBDicDgsl8uQLd8TXDBd18NMDpvUdd3wCb55pJ+YppnP523bDq+OmS/qSf3cZpXYXwAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (2, 1), Qty: 375\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABxUlEQVR4nO2UIc/iQBCGdy/3D9i1VNJamuCKxdJqLKvLf0BjgdRhaakkQdFQ14R8omXrsFAqCZhO90Qv3OU74KNH1eUesWJn9t13ZyeDPgT6EKhavlWs9y+iqupwONztdgCQ5zkABEEwm80URbmbjx8JMcZkWdY0DSHUbDaFEBhjIYRlWa7rrlarJyYeiuZ5LoS4Xq+c881mE8dxmqaLxeLl593Dtu0sy4IgeEvlE5TS/X6fJEm9Xi979mGfnk4ny7JqtRohpKzo9ycxjDHGWFEUjH+WnnN+uVzK3vGL4vkAkGXZbbVt2zCML8/e/31Kqed5jUZju91yzn3fRwj1+31CiCRJQohWq1XataZpADCfzz/tE0JM01yv1wAQhuGj5v9LGGNBECRJoqpqlbqEkCiKDofDK1UuQbvdLvyaplmlbuE3y7LfN98d0mma+r5/a+RqUBTleDyGYfhS9mAw6PV6z3MkSXIc53w+y7L8taJhGAAwHo//DFFKGWOMseVyWXjUdf0lm4ZhCCEAIEmSyWQynU49z3Mc5zb58zyPomg0Gt0dNw8L3Ol0ut0uQkjXdUop5zxN02J1XRchFMfxW8PlP6X4AVG2/8iDn7HJAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (4, 0), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB/UlEQVR4nO2Vz+v5cBzH34YVTUoOU0OZdtiBctpFaYqSXBzwh5B/gaNywx+gKH+A3JTDHJQfJWl+nFYrUWR72fewvp+UfT/xoT6X7+O0Xq9Xjz3fa68NaQhp6MNgnxb+57eIxWK1Wm2/32uaJghCuVx+S0eS5HA4VFUVAERRXCwWkiSpqlooFH5odLvdgiAAwHq9TiQSTqcTIeT1esfjcbfbxTAsl8sxDGMymV6QVioVANhutziO39dpmqYoKp1OAwAA2Gy2Z435fF5RFEmSCIJ47LIsK8syAHS7XcOkxhsVCoUwDJtOp6fT6bG72+30i+PxqGkGS24spWkaIVStVg27yWRSP3W73TYcMMBut1+vVwAIh8OPXRzHl8slABwOB7/fb2gwTmo2mw3rVquV5/lAIIAQarVaoig+mxTH8dVqBQDFYvG+7vF4SqUS/MXwHDrGbxlFUbPZjCCIfr/f6XRYlnU4HNFolCRJRVGcTudms4lEIrIsP5tUJ5PJjEYjPdTlcpnP541GIx6Pn89nAGg2m6/pvrBarRzHcRwXiUT0CsMw+m1SqdQPpY/wPK9LXS7XN2Ov/U6y2ex7qR7w+Xz6dg4GA4vF8s3kC0mDwaD+rer1eqqqvptRJ5fLAcDpdPrXanzxQlL9gU4mEwB4K909t9sNAOr1+seMv88fI4/prRwBUIcAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (1, 2), Qty: 408\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACS0lEQVR4nO2VTUsyURSA7/ixiRkdKqiNIEQqBOJCCQfaShDqRhJFcuMI4cI/oJCLfsD8gQFR9AeIDKJo4saN4MdCKAYhbFWBUDTqePBdDK+INjVamxfeZ3U599znHu49dwaheQjNQ+hXUf2u7l8DWw/hOG4wGK6vrxFCLMu22+2fboLjeCqVgr9Mp9NMJrO7u/sj6e3tLazx9PTkcrm2l9I0DQCz2YxhmGg0GovFXl9fAUAQhJubm52dnW2kxWIRAPL5/CJydnb2/PwslZzJZLRa7cbS+XwOACcnJ8tBiqLq9brkzWazGo1mM2m5XAYAo9G4Ej89PX15eZG8l5eXX0tWX1S/31+MI5FItVr1+/1+v99oNC7O5Pj4eLNKr66uAMDj8VgslvF4vN4JADAYDHw+n16vVyrV6XShUAjH8aOjo9Fo9KlU4v39PRAIbNwPFxcX6XS6UCh8oe50Oiu3qgi1Wk2SJEmSZrPZZDJJY4Zh3t7eJG+pVLLZbEp1+/v7FEXJzVIU1el0JC/HcYqMbreb53lBELxer1wOQRAPDw8AMBqNzs/Pv5cGg0FBEKQn63Q65dIcDocoigBQq9UWQdkvfy6XSyaTCCEMw1Qq2TSr1YphGEKo2+1+XylCSKfTVSoVAOB5nmVZk8m0PBuPx3u93sfHh3SsDMMokiKEcBzneV5aKYriZInlxmo2m3t7e0qlEuFwmOO44XC43qeNRiORSBwcHCznf/I7kePw8JAgCJqm7+7u7Hb7/f19q9V6fHycTCab1fifLfgDGnCYB6+a458AAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (4, 3), Qty: 396\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABwElEQVR4nO3UsYrqQBQG4CFRG0VUsBIMWARFCYGg4BNYpdZCsLCxVFjQzqCtlaU+QyoLbQWjnYhNsBJBEhCNJiAqxJMtBmR3WZabe6fY4v5dAufjz5khCL056M1BREOR5f7nd4RhmF6vN51OM5kMAY5l2X6/f7lcAAAA9vs9z/OiKHIc9zccRVHpdFrTNPgc0zQBQFEUinJ5L6PRaKfTeUHn89m27Y/06XTyeDzu0MFggIcfj0etVisUCvP5/CUeDod8Pu+CoyhKlmXbtm3bXi6XgiC0Wi1VVT/WnEwm7jrW63U8qapqsVi83W5fdrrZbBiGcSF6vV5d178ox+NRkqTZbIYfm82mu5o0TSuKgoev16thGN1uNxwOcxyHKy8Wi2Aw6A5FCIVCoXK5XCqVkskkfhMIBGRZBgDLsliWdS1+m0qlgrsPh0MyYiQSWa/XALDb7Xw+Hxn0teJqtUpGTCQSlmUBwGg0ommagBiLxbbbLf7weDxOQEQIiaIIAI7jtNttMmIulzMMAwDu93s2myUg+v3+8XiMz0fX9VQqRQBtNBpY1DSN5/k/H/zp//p8Pk3TlCRJEITVavWPFX9l3gEdkUpp1/EgFAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (3, 2), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABq0lEQVR4nO2UvYrCQBSFZ+JPsLAQwdJSSGcCgp0gjJUELCwEidj4BOYFbBTfQBsbOxtFQW2sFKxsFOzSREREQTBY3SFbDLuILLMkBnaLPdVw4HycudwZhOwyssvIUwne4n5bsixPJhNKqWVZiqJ4A53NZgAAAKfTqd/ve0DMZrPn8xkAWq1WIpFIp9PvEqPR6OVyAYDhcOj3+z3oiBDK5/Ps4o4K/rBSmUwGYzwajdbr9Xv1PhWLxXa7HQAQQhwFeU01TZMkybKs6/XqGVSSJISQYRibzcYRlKfj8UgprdVqToO8phhjQRBUVW232+xF2bZNKTUMo1qt+nw+903hSdvt9uus67p76O12m06npVKJECKKIiFkMBgwbrFYdAxtNpuU0k6n8+KHw+H9fg8A9Xr92yBvpmyTUqnUi3+/35fLJSfIgz4eD0EQAoGAKIrPvizLqqpijDlZnnq9HgB0u91QKMSceDy+Wq3YTCuVihtoJBIxTZNxc7lcoVA4HA4AYJpmo9Fw2RQhpCgK4wIA27D5fO7B/59MJsfjMQAsFgtd14PB4LvEf/0JfQCzTO9oVEmQAAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (3, 4), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABMElEQVR4nO1U0Y2FIBDcXK4Mtw6DdejahkAdJNaBFvAqQOsA+9j7IEeM+C5PH8n7uflcYJiF2YEH8AMYiuKrLN2HoJRiZiFESVLvfWFSRGRmpdSNs08/qus6ANi27b6uHM45ZkbEYoyxd+/9vePfp9VhGABgmqZ8SQgR5dd1Pc/zsiyvXmWtZWYiyhmjJRJOvXFtoqSUUabWummaEIIx5i2lQoioLtXjtreUSikBoO97a+2+nr/SOWkIAQCqqkoVROy6blmWA+MFREvtW4udHgbMOXdqu6dKo1cOLPsBIyIhxDiOF8QSURRrrU1flAaMiLz33vvLI6eUOriSiIgoPsX9AENEYwxncM4ViIUk2RiTe+iAV32acgAR27b9u/HzQMmhtYbfkF3XtXDO/uMz+AFHmOcEHcwxuwAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (0, 2), Qty: 423\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB/0lEQVR4nO2TO6viUBRGM5MoBAtbIVgoovhqxAEDKohYWCQINha3s9VG/4SNFqaxEHxhbaGtQkQQy0RBsE4aJYUSJZgcnSJMkMTH9c5tBmaV+3x75eNwAkHXD+j6AX0rP79X96/x45M5GIYJgiiXy/V6XZZldTiZTA6Hw9c/jqIoMNBoNN4WWSyWeDweiUQeSWVZFgQBx/E3pNVqFQCw3++z2SyCIJ1Ox+gFAESj0c92rFarkiSpaxRFQRCUTqcZhlEURSddrVafklIUdbs2n8+dTqd61G63AQC1Wm06naqnoigSBPFMh6JopVI5n8+6OqlUSg1YrdZwOGwymTAMWywW6mmz2XwmzWQyxltjGMbr9RrDLpdLDcxmM4fDoc31v6nZbNZNBEHI5XLr9doo1cI4jgeDwYdNL5eLrma/338UXi6XWowkyYdNdQwGg2Kx+DzzmtumLMsiCGLM+Hw+nudPp5P2vFqtFgzDWuDOjsb1elUU5XaSz+dDoVAsFrPZbNrweDx2u10AwDOR1nSz2fz6w2g04jhOFEXj23jxSCEI6vV6xrUn0DRtt9tfSO++07tst1uapm/v4SGBQIBl2ZdGnueTyeRrnYbb7eY47pFOkqTdbpdIJN4wqng8nrt9x+NxqVR6W6fh9/sLhYLqGg6HJEmSJIlh2NeN//kbfgO7uDCbn5syGAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (0, 0), Qty: 423\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAA5klEQVR4nO2UMQ5EUBCGJ6tU6RSicAc30Ite7wwO4hQKSidAIZFINGoqlYSCzGT2AAoj3ha72a/+35cvLy8PGIBBMS/Vwl9jGAYi0nVdfuT6TpmZmYMgeBB2oq5rRERElVLf95umOY7DNE3F3n3fkyQR7kXv1LZtTdPCMHwQdqJtW0RclkW4F5VmWXYrQiR1HOeWVMQ0TYhIRFEUKZO6rlsUBTOnaapMCgBxHBPROI6S8Ue+vu+RSun7nojmebYs63IsLV3XFQAMw/A8T5k0z3Ph8ob0FlJpVVVlWW7b1nXdJzr+XPAGHrBgOg1RTEcAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (1, 0), Qty: 409\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABbElEQVR4nO2UoY/CMBSHy+RqW4te7fgj0MOCRswQ7CaZxi5YkGyaBLXpJZuhIIcEMTUkj55oriEcQazN5cR9oqJpv/zy3muRQEggw1imhf/8JpTS+Xye5/nj8QAAtSZJMhqNOkp3ux0A3O/3n2vbto7jfL7+fqQIIZZlNU2z3+993y/Lsqqqpml6vR7GeDabdUnqeV4cx67rqh3GWJ7nMm9H6QsYY865rGxRFAaMYRhyzlVNPc/TNQ4Gg+fux3FMKdWV2rZ9OBxU9y+XS13XQRB0HyxFGIZ1Xb/M7HA41PUSQlzXXSwWqr5ZlulKFZTS9Xot806n07dnuvynjDEhhBDidDrpJfwmiiJZ0/F4bEBHKV0ul3ISrtdrv9/XNTLGjsejzMg5N5Bxs9m0bSszJkli23ZHEcY4TdM0TQFACAEA5/NZ941GUfT8k263W0KIlhEhtFqtAOB2uxVFYeBRSiaTSZZlQRCY0f1RvgAtyhrNWFmmGgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (1, 4), Qty: 408\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABs0lEQVR4nO2UPYvCMBjHgxUnU3CVioIvUCehg9jJoZu4OFRFt0JXNz+FII5dXJ0E4yTo4gc4EZw6CGJFXRQVQcTmvCF3IlbPu9jpuN+Ylx9P8vwT0AHnDjgDW3HYq/vnOxRFeb9C1/VyuczzPL2x3W4fDgdsoVKpUBqTyeR+v8cYD4dDhBBCKB6PK4qCMT4ej8Vi8anBaR0ajUYIIZZlVVVdLBZkkGVZAIDT6fR4PDTS1WpVKBTurjZNc7fbPZX+CAihpmkY42q1ao8xkUhsNhvSqHw+/6rO5XKVSqVLDAzDeClSAAC/3z+bzW4iNR6Pc7kcvTQYDFpzijE2TXMwGMRiMRopx3FvX2QyGUmSJEnq9XpEPZ1OBUGgL/kaURQbjQbxTiaTUChkj5dhmFarRbyiKNojJTSbTdK3cDhsnaX8+bvdLgAgEAhEIpGXqrvA87xhGOQGUqmUdcGdt/8It9stCEI6nZZl2ev1AgC22+16vaasKxqNaprW7/evA6vruizLv3MxDOPz+er1+ul0usn/crlUVRVC+Gjvw+NDCLPZLMdxDsdnM+fzea1Ws/Ov+qN8AAlzI19JSn6UAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (3, 0), Qty: 398\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABm0lEQVR4nO2VravCUBTAj+6x6kcQZCKsDQaW/Q9q8G+QaVwxyRAMJi1rBrNtySYGEZMYXFEYgqAiWmZQEJRZzn1h8BCc41296fF+6XLPOT/OvdwPIAAEGBNmLfznT5FIJEqlUr/fR0RCyHq91jSN47h3XKlUqtFo2LZ9vV4RERFvt9v5fPbGxWKRTicIQqfT+anfbDaWZY1Go2w2KwjCcrlExHK5TCeVZdl1XUR0HEeSpEgk8hhVVRURK5UKnRQAarVaPp8XRfE5VCgUEHE4HAaUf/nOtlqtVwXxeBwAVqtVgJT67qfTaQBot9u0hS/heX42m/V6vVAoFJBG16mqqoqiWJZFSNB76b+nz0iSNBgMTqcTANzvd1EUt9stVUP+Uu+ceVwul/F4nMvlPvUahuEZXdedz+eIaNu2b+Zvl89xXCaTWSwWuq4fj8dkMlmtVqfT6UdtNptNRNR1/SPLI9FodDKZmKYZDrP70wzD2O12siwzMyqK4jiOpmnMjADQ7Xb3+30sFmMpPRwO9XqdpREATNPkeZ6x9A2+AYpjxtYtErTMAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (2, 3), Qty: 375\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABkklEQVR4nO3Vr6vCUBQH8MOzTIU1LYKCGsVgNMv+Ag2CTbNJcCZB/4MhsrogQ5NiMxoMhrFmMC0qiMGNG9RzX7g8GW/O59Xx4MH7pnF+fBYObAAtCi0KgeYjWO4/v55Op0MpHY1G3pYkSYg4n8/9dn2vH4lEKKW2bXtb2WyW0YVCgQ+tVCoAYJqmt5VOpwGAEHI6nfzW70QURcuyELFcLntfttvtEHGz2XCIAFAsFhEREfP5vLsuCMJisWAtwzBeRMPhsLuuKAp+RZZlPnQ4HHrRbrd7Pp9Z/Xg8xuNxv/X7hwqFQt8qtVqt3W7f6qvVar/f86GmabLLplIpAEgkEqqqCoJwG7Asy098FE3TEHE8HjcaDdu20ZXL5SJJ0itoqVSaTqfX65VBjuPous6e1+v1K+It9Xp9NpspipLJZJrNJkMHg8FbqDuGYTC03+8HhvZ6PUTcbrfRaPTxJMfvJJfLAQAhxHGcwNDD4QAAk8nkx0kONJlMAgAhJEg0Fos9OcmB3v1gv4tWq9Xlcvn8/F/IJ9K85CQq8QdpAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (3, 1), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACI0lEQVR4nO2UsetpYRjHXzedCVEkKZMoSgYxKYMyWBhktFEyyGKimBSDAVmU0SSJIn8DDpJjVESR0jln4PScc4dz+3V+/I7f7Xbq3uF+x+/zPp/3+z49vQjnEM4hafVDYt7fkVqtns1m2+1WMqJGo5lOpwBAEITERIZhyuWyNNBSqQQAAFCv16UharVaHMcB4Hq9Wq1WaaCLxULimAghlmUB4Ha7mc3mPyPIXi2O41iWvV6vOp3utWqxWBQKhdBZrVYMw/xW0vP5/OT7/f7BYHC5XOCzJpNJPp83mUzvkrIsy3HcU1KlUjkejz0eD0JoPp+TJLlery+Xi9PpDAQCGIbtdrt2u10sFt8lrVQqQrNarfK5Op2OSqUSllKp1Gaz4avfPB/HcaHZ7Xb5Nq/X+9qSTqeFUNFfymg0il4roo9fQhSKYdiXK5VIJMRaCoWC6IWJRIJ/y+l0ikQivGkwGHK53OFwoChqOBz6fD5hS6vVAoB0Oi0Klcvl8/mc5z5NNpPJHA4HAKAoKhqN8qbNZiNJEgDC4bAolM/FQx+PR6PREM7B5XLVarXhcEjT9Gg0SiaT+/0eAGKxmEz2a0G/2FOEkEwmi0aj2WzW4XAghGia3u12rVbr4wCGYW63m4+2XC6DweDxeOQ47h2UVygU8ng8wWDQbre/VgmC6Pf7OI73er37/f4p07spIIQQksvler0+Ho8/+c1m83g8ftv+X/+wfgKJ8kcg4T3iXgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (4, 1), Qty: 397\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB4klEQVR4nO2Vr8vqUBjHzxWZRVGsGpYMhgkiGJStiVFhrihoEgRtC7YZxeRfIMKw2YZisojB9qaBOGToLAoyBcEfnMcbBjIQ793ed+XC/bRz+PI5zznPAwd9PdHXEzmLy2Hff/4lflnMhUIhiqJYliUIgiRJVVWbzeZut/vmsel0ejKZnM9nAMAmer2e2+22rYvH45IkXa9Xw7JcLufzuSAIg8HAOINhGBs6l8vF8/x+v8cY3+93URRzuZw5MBqNAKBer9uQ8jyPMQYAWZZpmn4PGNJ+v+/z+SwZy+UyAADAeDz2eDzvgUgkomna8/kEAEEQLElbrRbGeLFYvBtJkkwmk8Ph0LgHxjgajb4bPraPIAiaplVVfTweuq5zHMeybCKR8Pv9r4woiqvVylKlHMfdbrdXLZfLZb1ev5YvAKBarVoyGlQqle12a7YoitJut7PZbK1Wk2UZY6xpmtfrtSE1SKVSxWKRoijzJsMwRhtLpZJt4yckSfrUxm8SDocPhwPGuFAoOGNECAmCgDE+Ho+BQOAPMXt/VDAYRAgpiqLr+g9qMxGLxU6nEwB0u11njAih8XhsjG0+n3fGSFGUMbadTuevYatv2mg0EEKbzWY6nf6oOjOpVGo2m2UyGceMdvkN2RIpHWpzxnQAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Group: (1, 3), Qty: 408\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABrklEQVR4nO2UMYvCMBiGc7UVRKuiRQcdLHZy6eDipovQTXcXQVCyixQUnRwc9A+4iD/ASdBVBx3FQf0BUqcOUgOi4cMbCsKdWPTocAf3bPny5kkCX4JQ9YaqN2QrjL26f/4UHxZzLMuKolgoFDwej1kZjUbb7fZ4PP5kK1mWa7XafD6HB3a7XT6ff9tYLpcXiwUAaJrW7/dFURQEQRCERCLRaDQopefzeTgcvmFMp9O6rp9OJ4yx0+l8DGCMAYAQkkwmX5UahgEAzWbzWYDjuOVyCQCKojzLfH+mPM8jhAghzxZQSi+Xy6tnNKlUKplMJhgMPgvE43FCyH6/v7eEDbRaLQDodDq2GVVVBYBer+dwOOwxZrNZANhsNpFIxB6jqqqHw2EymUSjURt0oVBoMBhcr9fxeGzbrc3Hulqtcrmc3++PxWKPmUAgIMvyfWj1ofA83+12i8Xi/YC6rjMMQwjhOM6saJo2nU4xxi6Xy+xxhBBrIU2lUqVSqd1ur9fre9EwDEVRGIZxu92z2axer3u9XkqpRWt/gWVZSZKs0z6fDyEUDoclSXpJ+s8v4hMxU7R4ak5MywAAAABJRU5ErkJggg==\n"},"metadata":{}}],"source":["from IPython.display import display\n","for (k, v) in valset.group_partition.items():\n","  print(f\"Group: {k}, Qty: {len(v)}\")\n","  img = T.ToPILImage()(valset[v[0]][0])\n","  display(img)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":45},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1702409981538,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"KwF4cThKMLxC","outputId":"4037081f-c997-434e-e41d-b2e7847e11e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=28x28>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABrklEQVR4nO2UMYvCMBiGc7UVRKuiRQcdLHZy6eDipovQTXcXQVCyixQUnRwc9A+4iD/ASdBVBx3FQf0BUqcOUgOi4cMbCsKdWPTocAf3bPny5kkCX4JQ9YaqN2QrjL26f/4UHxZzLMuKolgoFDwej1kZjUbb7fZ4PP5kK1mWa7XafD6HB3a7XT6ff9tYLpcXiwUAaJrW7/dFURQEQRCERCLRaDQopefzeTgcvmFMp9O6rp9OJ4yx0+l8DGCMAYAQkkwmX5UahgEAzWbzWYDjuOVyCQCKojzLfH+mPM8jhAghzxZQSi+Xy6tnNKlUKplMJhgMPgvE43FCyH6/v7eEDbRaLQDodDq2GVVVBYBer+dwOOwxZrNZANhsNpFIxB6jqqqHw2EymUSjURt0oVBoMBhcr9fxeGzbrc3Hulqtcrmc3++PxWKPmUAgIMvyfWj1ofA83+12i8Xi/YC6rjMMQwjhOM6saJo2nU4xxi6Xy+xxhBBrIU2lUqVSqd1ur9fre9EwDEVRGIZxu92z2axer3u9XkqpRWt/gWVZSZKs0z6fDyEUDoclSXpJ+s8v4hMxU7R4ak5MywAAAABJRU5ErkJggg==\n"},"metadata":{},"execution_count":11}],"source":["T.ToPILImage()(valset[v[0]][0])"]},{"cell_type":"markdown","metadata":{"id":"CC12sKhEn3HA"},"source":["## Contrastive Loss"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QgM-GhGrn9hB","executionInfo":{"status":"ok","timestamp":1702409981538,"user_tz":480,"elapsed":13,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":["## https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ntxentloss\n","## https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#selfsupervisedloss\n","criterion = losses.NTXentLoss(temperature=Config.temperature)\n","criterion = losses.SelfSupervisedLoss(criterion)"]},{"cell_type":"markdown","metadata":{"id":"_CVN-T1PnzcZ"},"source":["## Model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"hVxfbTDSZxDZ","executionInfo":{"status":"ok","timestamp":1702409981538,"user_tz":480,"elapsed":13,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":["## https://github.com/berfukaraca/SSL-Cifar10-ContrastiveLearning/blob/main/colab_notebooks/SSL_SimCLR_Cifar10_CL.ipynb\n","\n","class SimCLR_model(nn.Module):\n","    def __init__(self, base_encoder, projection_dim, n_features):\n","        super(SimCLR_model, self).__init__()\n","        self.n_features = n_features\n","        self.base_encoder = base_encoder\n","\n","        self.projectionHead = nn.Sequential(\n","            nn.Linear(self.n_features, self.n_features, bias=False),\n","            nn.BatchNorm1d(self.n_features),\n","            nn.ReLU(), #non-linearity\n","            nn.Linear(self.n_features, projection_dim, bias=False),\n","            nn.BatchNorm1d(projection_dim),\n","        )\n","\n","    def forward(self, x_i):\n","        out = self.projectionHead(self.base_encoder(x_i))\n","\n","        return out"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1702409981892,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"},"user_tz":480},"id":"Oe-1cXa2ZxuL","outputId":"f11ccbcf-e420-4c4d-c622-b7261c1f855b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SimCLR_model(\n","  (base_encoder): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n","  )\n","  (projectionHead): Sequential(\n","    (0): Linear(in_features=1000, out_features=1000, bias=False)\n","    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Linear(in_features=1000, out_features=128, bias=False)\n","    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n",")"]},"metadata":{},"execution_count":14}],"source":["## model and optimizer\n","model = models.resnet50()\n","# print(model)\n","clr_model = SimCLR_model(model, Config.projection_dim, 1000) ## output dimension of resnet18 is 1000\n","optimizer = optim.Adam(clr_model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n","## cosine annealing lr: https://arxiv.org/pdf/1608.03983.pdf\n","scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.T_max, eta_min=Config.min_lr)\n","\n","clr_model.to(device)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPPiQ-wvlj5s","executionInfo":{"status":"ok","timestamp":1702459090714,"user_tz":480,"elapsed":49108831,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}},"outputId":"6d906a3f-931f-4afe-94fa-1e99f93936ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 6.2463884353637695\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 0train loss= 5.68171211506458\n","epoch 0val loss= 1.2345950083529695\n","\n","Epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.280658721923828\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 1train loss= 5.176342657271852\n","epoch 1val loss= 1.2416574396985642\n","\n","Epoch: 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.154621601104736\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 2train loss= 5.0382743186139045\n","epoch 2val loss= 1.262875666009619\n","\n","Epoch: 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.981607437133789\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 3train loss= 4.968131691851514\n","epoch 3val loss= 1.267737160337732\n","\n","Epoch: 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.891085624694824\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 4train loss= 4.916219064529906\n","epoch 4val loss= 1.275575556653611\n","\n","Epoch: 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.830238342285156\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 5train loss= 4.885768804144352\n","epoch 5val loss= 1.2499474730897457\n","\n","Epoch: 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.8570451736450195\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 6train loss= 4.852043856965735\n","epoch 6val loss= 1.2583219256806881\n","\n","Epoch: 7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.843724727630615\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 7train loss= 4.8313556655924375\n","epoch 7val loss= 1.2700381240946181\n","\n","Epoch: 8\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.812040328979492\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 8train loss= 4.8153961668623255\n","epoch 8val loss= 1.2647719104239281\n","\n","Epoch: 9\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.834261894226074\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 9train loss= 4.81246421945856\n","epoch 9val loss= 1.2667030692100525\n"]}],"source":["import os\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    clr_model.train()\n","    s=0\n","    tot_loss = 0\n","    total = 0\n","    batch_idx=0\n","    i=0\n","    batch_num =1\n","    val_loss = 0\n","    for batch_idx, (images, _) in enumerate(dataloaders['train']):\n","        batch_tot_loss = 0\n","        if (batch_num % 50 == 1):\n","          print(\"in batch\", batch_num)\n","        batch_num=batch_num+1\n","        images, augmented_images = images[0], images[1]\n","        images, augmented_images = images.to(device), augmented_images.to(device)\n","        optimizer.zero_grad()\n","\n","        images_emb = clr_model(images)\n","        augmented_images_emb = clr_model(augmented_images)\n","        loss = criterion(images_emb, augmented_images_emb)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","        tot_loss += loss.item()\n","        batch_tot_loss += loss.item()\n","        if (batch_idx % 200 == 1):\n","            print('batch loss=', batch_tot_loss)\n","    scheduler.step()\n","\n","\n","\n","    # Validation\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (images, _) in enumerate(dataloaders['eval']):\n","            val_batch_loss = 0\n","            if (batch_num % 50 == 1):\n","                print(\"in batch\", batch_num)\n","            batch_num=batch_num+1\n","            images, augmented_images = images[0], images[1]\n","            images, augmented_images = images.to(device), augmented_images.to(device)\n","\n","\n","            images_emb = clr_model(images)\n","            augmented_images_emb = clr_model(augmented_images)\n","            loss = criterion(images_emb, augmented_images_emb)\n","\n","\n","\n","            val_loss += loss.item()\n","            val_batch_loss += loss.item()\n","        if (batch_idx % 200 == 1):\n","          print('batch loss=', val_batch_loss)\n","\n","    print(f\"epoch {epoch}train loss= {tot_loss/len(dataloaders['train'])}\")\n","    print(f\"epoch {epoch}val loss= {val_loss/len(dataloaders['train'])}\")\n","\n","    if epoch % 10 ==0:\n","        save_dir = f\"/content/drive/MyDrive/CS260D_final_project/model_parameters/\"\n","        if not os.path.isdir(save_dir):\n","          os.mkdir(save_dir)\n","        modelstate_save_name = f'modelstate_bs_restart_r_{Config.spurious_correlation_strength}_SMALL_epoch{epoch}.pt'\n","        path_modelstate = os.path.join(save_dir, modelstate_save_name)\n","        torch.save(clr_model.state_dict(), path_modelstate)\n","\n","        parameters_end = {\n","            'net': clr_model.state_dict(),\n","            'epoch': epoch,\n","            'loss': tot_loss\n","        }\n","        paramsend_save_name = f'paramsend__bs_restart_r_{Config.spurious_correlation_strength}_SMALL.pt'\n","        path_paramsend = os.path.join(save_dir, paramsend_save_name)\n","        torch.save(parameters_end, path_paramsend)\n","\n","start_epoch=0\n","max_epoch=10\n","\n","#for epoch in range(start_epoch, start_epoch+max_epoch):\n","#  train(epoch)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"0YT8wSIIimLi","executionInfo":{"status":"ok","timestamp":1702459090715,"user_tz":480,"elapsed":7,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"3l9W9JBQsFYV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702459090943,"user_tz":480,"elapsed":233,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}},"outputId":"bc52ba9f-d05b-4cf9-c312-a41abf3e2f60"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":16}],"source":["## Load the pre-trained model parameters\n","paramsend_save_name = f'paramsend__bs_restart_r_{Config.spurious_correlation_strength}_SMALL.pt'\n","save_dir = f\"/content/drive/MyDrive/CS260D_final_project/model_parameters/\"\n","path_paramsend = path_paramsend = os.path.join(save_dir, paramsend_save_name)\n","checkpoint = torch.load(path_paramsend, map_location = torch.device(device))\n","loss= checkpoint['loss']\n","epoch = checkpoint['epoch']\n","\n","start_epoch=epoch+1\n","clr_model.load_state_dict(checkpoint['net'])"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"W-FFz1johNEl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702459090943,"user_tz":480,"elapsed":5,"user":{"displayName":"TERRY PEDERSON","userId":"15628522916344925635"}},"outputId":"1f6b9ca1-e347-4341-a001-7f1eb6ca5c30"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":17}],"source":["epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2-vw6YyCjo2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b53951e-b81c-44c6-880c-2c2c3de376b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.219264030456543\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 1train loss= 5.238217721594141\n","epoch 1val loss= 1.2342995760288644\n","\n","Epoch: 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.272327899932861\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 2train loss= 5.196777780005273\n","epoch 2val loss= 1.2295703507484275\n","\n","Epoch: 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.1971540451049805\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 3train loss= 5.136139425825565\n","epoch 3val loss= 1.2515533388929163\n","\n","Epoch: 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.2241082191467285\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 4train loss= 5.055524303558025\n","epoch 4val loss= 1.249866635241407\n","\n","Epoch: 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 5.014698028564453\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 5train loss= 4.990499458414443\n","epoch 5val loss= 1.2539364619457976\n","\n","Epoch: 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.948493003845215\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 6train loss= 4.935482245810489\n","epoch 6val loss= 1.261129413513427\n","\n","Epoch: 7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.918099880218506\n","in batch 51\n","in batch 101\n","in batch 151\n","in batch 201\n","epoch 7train loss= 4.897286042254022\n","epoch 7val loss= 1.2723502389928127\n","\n","Epoch: 8\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["in batch 1\n","batch loss= 4.864962577819824\n","in batch 51\n"]}],"source":["start_epoch = 1\n","max_epoch = 10\n","\n","#for epoch in range(start_epoch, start_epoch+max_epoch):\n","#  train(epoch)"]},{"cell_type":"markdown","metadata":{"id":"slnn2BoJkcnr"},"source":["## PCA and 2D Scatter Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRJyWIydMkjI"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BvNuyw5sHes"},"outputs":[],"source":["## https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.decomposition import PCA\n","\n","\n","torch.cuda.empty_cache()\n","device = 'cpu'\n","clr_model.to(device)\n","\n","embeddings = []\n","targets = []\n","\n","for batch_idx, ((images, _), labels) in enumerate(dataloaders['eval']):\n","  embeddings.append(clr_model(images).detach().cpu())\n","  targets.append(labels)\n","  torch.cuda.empty_cache()\n","\n","\n","# clr_model = None\n","\n","embeddings = torch.cat(embeddings).detach().cpu().numpy()\n","targets = torch.cat(targets).cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JX6E60Ds9e5v"},"outputs":[],"source":["import pandas as pd\n","comp = 100\n","pca_2d = PCA(n_components=comp)\n","PCs_2d = pd.DataFrame(pca_2d.fit_transform(embeddings))\n","PCs_2d.columns = [f\"PC{i+ 1}\" for i in range(comp)]\n","pca_embs = pd.concat([pd.DataFrame(embeddings),PCs_2d], axis=1, join='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmO_3JHgRo48"},"outputs":[],"source":["pce = pca_2d.fit_transform(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q91_BfhVRt2K"},"outputs":[],"source":["pca_2d.singular_values_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rU3kQDF_Oob"},"outputs":[],"source":["pca_embs['true_label'] = targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSmCReMU-t6e"},"outputs":[],"source":["clusters = {k : np.stack([PCs_2d.loc[i].values for i in v]) for (k, v) in valset.group_partition.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UkZz6yHAKVQ"},"outputs":[],"source":["import plotly as py\n","import plotly.graph_objs as go\n","from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","\n","traces = []\n","\n","for (k, v) in sorted(clusters.items()):\n","  traces.append(go.Scatter(\n","                    x = v[:, 2],\n","                    y = v[:, 3],\n","                    mode = \"markers\",\n","                    name = \"True label {}, Subgroup {}\".format(k[0], k[1]),\n","                    text = None))\n","data = traces\n","\n","title = \"Visualizing Clusters in Two Dimensions Using PCA\"\n","\n","layout = dict(title = title,\n","              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n","              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n","             )\n","\n","fig = dict(data = data, layout = layout)\n","\n","iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tfd5iaLnqrbr"},"outputs":[],"source":["for (true, spur), idxs in valset.group_partition.items():\n","  for i in idxs:\n","    pca_embs.loc[i, 'spurious_label'] = spur\n","pca_embs['spurious_label'] = pca_embs['spurious_label'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZEDbuEHh78s"},"outputs":[],"source":["valset.group_partition.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B7qPSy1aUAy"},"outputs":[],"source":["true_label_clusters = {\n","    # True label: {group, cluster_dim}\n","}\n","for l in pca_embs['true_label'].unique():\n","  true_cluster = pca_embs[pca_embs['true_label'] == l]\n","  true_emb = true_cluster.iloc[:, :128].values\n","  pca = PCA(n_components=3)\n","  PCs = pd.DataFrame(pca.fit_transform(embeddings), columns = ['PC1_TL', 'PC2_TL', 'PC3_TL'])\n","  PCs = pd.concat([PCs,true_cluster[['true_label', 'spurious_label']]], axis=1, join='inner')\n","  true_label_clusters[l] = PCs\n","  # pca_embs = pd.concat([pd.DataFrame(embeddings),PCs_2d], axis=1, join='inner')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNbebi9bIpzD"},"outputs":[],"source":["spurious_label_clusters = {\n","    # Spurious label: {group, cluster_dim}\n","}\n","for l in pca_embs['spurious_label'].unique():\n","  spurious_cluster = pca_embs[pca_embs['spurious_label'] == l]\n","  spurious_emb = spurious_cluster.iloc[:, :128].values\n","  pca = PCA(n_components=3)\n","  PCs = pd.DataFrame(pca.fit_transform(embeddings), columns = ['PC1_TL', 'PC2_TL', 'PC3_TL'])\n","  PCs = pd.concat([PCs,spurious_cluster[['true_label', 'spurious_label']]], axis=1, join='inner')\n","  spurious_label_clusters[l] = PCs\n","  # pca_embs = pd.concat([pd.DataFrame(embeddings),PCs_2d], axis=1, join='inner')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SI9BPr7-463R"},"outputs":[],"source":["import os\n","\n","save_dir = f\"/content/drive/MyDrive/CS260D_final_project/plots/\"\n","for k, v in true_label_clusters.items():\n","  traces = []\n","  for s in np.sort(v['spurious_label'].unique()):\n","    group = v[v['spurious_label'] == s]\n","    traces.append(go.Scatter(\n","                      x = group['PC2_TL'],\n","                      y = group['PC3_TL'],\n","                      #z = group['PC3_TL'],\n","                      mode = \"markers\",\n","                      name = f\"Subgroup {s}\",\n","                      text = None))\n","  data = traces\n","\n","  title = f\"Visualizing Clusters in Two Dimensions Using PCA - True Label {k}\"\n","\n","  layout = dict(title = title,\n","                xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n","                yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n","              )\n","\n","  fig = dict(data = data, layout = layout)\n","  save_name = f\"simCLR_{Config.spurious_correlation_strength}_SMALL_TL{k}_PCA_plot.html\"\n","  if os.path.isdir(save_dir):\n","    go.Figure.write_html(fig, os.path.join(save_dir, save_name))\n","  else:\n","    os.mkdir(save_dir)\n","    go.Figure.write_html(fig, os.path.join(save_dir, save_name))\n","\n","  iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X51r5pS9LJKz"},"outputs":[],"source":["import os\n","\n","save_dir = f\"/content/drive/MyDrive/CS260D_final_project/plots/\"\n","for k, v in spurious_label_clusters.items():\n","  traces = []\n","  for s in np.sort(v['true_label'].unique()):\n","    group = v[v['true_label'] == s]\n","    traces.append(go.Scatter(\n","                      x = group['PC1_TL'],\n","                      y = group['PC3_TL'],\n","                      #z = group['PC3_TL'],\n","                      mode = \"markers\",\n","                      name = f\"True Label {s}\",\n","                      text = None))\n","  data = traces\n","\n","  title = f\"Visualizing Clusters in Two Dimensions Using PCA - Spurious Label {k}\"\n","\n","  layout = dict(title = title,\n","                xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n","                yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n","              )\n","\n","  fig = dict(data = data, layout = layout)\n","  save_name = f\"simCLR_{Config.spurious_correlation_strength}_SMALL_TL{k}_Spurious_Label_PCA_plot.html\"\n","  if os.path.isdir(save_dir):\n","    go.Figure.write_html(fig, os.path.join(save_dir, save_name))\n","  else:\n","    os.mkdir(save_dir)\n","    go.Figure.write_html(fig, os.path.join(save_dir, save_name))\n","\n","  iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9zFtWoUHnQC"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","perplexity = 50\n","for perplexity in [35, 50]:\n","  spurious_label_clusters = {\n","      # Spurious label: {group, cluster_dim}\n","  }\n","  for l in pca_embs['spurious_label'].unique():\n","    spurious_cluster = pca_embs[pca_embs['spurious_label'] == l]\n","    spurious_emb = spurious_cluster.iloc[:, :128].values\n","\n","    pca = TSNE(n_components=3, perplexity = perplexity)\n","    ts = pd.DataFrame(pca.fit_transform(embeddings), columns = ['TC1_TL', 'TC2_TL', 'TC3_TL'])\n","    ts = pd.concat([ts,spurious_cluster[['true_label', 'spurious_label']]], axis=1, join='inner')\n","    spurious_label_clusters[l] = ts\n","  for k, v in spurious_label_clusters.items():\n","    traces = []\n","    for s in np.sort(v['true_label'].unique()):\n","      group = v[v['true_label'] == s]\n","      traces.append(go.Scatter3d(\n","                        x = group['TC1_TL'],\n","                        y = group['TC2_TL'],\n","                        z = group['TC3_TL'],\n","                        mode = \"markers\",\n","                        name = f\"True Label {s}\",\n","                        text = None,\n","                        marker = dict(size = 4)\n","                        ))\n","    data = traces\n","\n","    title = f\"Visualizing Clusters in Two Dimensions Using t-SNE {perplexity}- Spurious Label {k}\"\n","\n","    layout = dict(title = title,\n","                  xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n","                  yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False),\n","                  #zaxis= dict(title= 'TC3',ticklen= 5,zeroline= False)\n","                )\n","\n","    fig = dict(data = data, layout = layout)\n","    iplot(fig)\n","  # pca_embs = pd.concat([pd.DataFrame(embeddings),PCs_2d], axis=1, join='inner')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qyBnpT-_AHs"},"outputs":[],"source":["import os\n","\n","save_dir = f\"/content/drive/MyDrive/CS260D_final_project/plots/\"\n","for k, v in spurious_label_clusters.items():\n","  traces = []\n","  for s in np.sort(v['true_label'].unique()):\n","    group = v[v['true_label'] == s]\n","    traces.append(go.Scatter3d(\n","                      x = group['TC1_TL'],\n","                      y = group['TC2_TL'],\n","                      z = group['TC3_TL'],\n","                      mode = \"markers\",\n","                      name = f\"True Label {s}\",\n","                      text = None,\n","                      marker = dict(size = 4)\n","                      ))\n","  data = traces\n","\n","  title = f\"Visualizing Clusters in Two Dimensions Using t-SNE - Spurious Label {k}\"\n","\n","  layout = dict(title = title,\n","                xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n","                yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False),\n","\n","              )\n","\n","  fig = dict(data = data, layout = layout)\n","  save_name = f\"simCLR_{Config.spurious_correlation_strength}_SMALL_TL{k}_Spurious_Label_TSNE_plot.html\"\n","  if os.path.isdir(save_dir):\n","    go.Figure.write_html(fig, os.path.join(save_dir, save_name))\n","  else:\n","    os.mkdir(save_dir)\n","    go.Figure.write_html(fig, os.path.join(save_dir, save_name))\n","\n","  iplot(fig)"]},{"cell_type":"markdown","metadata":{"id":"CnoKCK0zzbyT"},"source":["## t-SNE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAPFpFOWPo5j"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","perplexity = 50\n","\n","#T-SNE with two dimensions\n","tsne_2d = TSNE(n_components=2, perplexity=perplexity)\n","\n","TCs_2d = pd.DataFrame(tsne_2d.fit_transform(embeddings))\n","TCs_2d.columns = [\"TC1_2d\",\"TC2_2d\"]\n","tsne_embs = pd.concat([pd.DataFrame(embeddings),TCs_2d], axis=1, join='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSBsAVzXzr4g"},"outputs":[],"source":["tsne_embs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTew3nk4zr6d"},"outputs":[],"source":["tsne_clusters = {k : np.stack([TCs_2d.loc[i].values for i in v]) for (k, v) in valset.group_partition.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D72d9gdo0TIZ"},"outputs":[],"source":["tsne_traces = []\n","\n","for (k, v) in sorted(tsne_clusters.items()):\n","  tsne_traces.append(go.Scatter(\n","                    x = v[:, 0],\n","                    y = v[:, 1],\n","                    mode = \"markers\",\n","                    name = \"True label {}, Subgroup {}\".format(k[0], k[1]),\n","                    text = None))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRYuHz5_0apY"},"outputs":[],"source":["data = tsne_traces\n","\n","title = \"Visualizing Clusters in Two Dimensions Using t-SNE\"\n","\n","layout = dict(title = title,\n","              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n","              yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False)\n","             )\n","\n","fig = dict(data = data, layout = layout)\n","\n","iplot(fig)"]},{"cell_type":"markdown","metadata":{"id":"mwn-TcME7b1E"},"source":["## PCA 3D plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAx47jjW5g7p"},"outputs":[],"source":["import pandas as pd\n","\n","pca_3d = PCA(n_components=3)\n","PCs_3d = pd.DataFrame(pca_3d.fit_transform(embeddings))\n","PCs_3d.columns = [\"PC1_3d\", \"PC2_2d\", \"PC3_3D\"]\n","pca_embs = pd.concat([pd.DataFrame(embeddings),PCs_3d], axis=1, join='inner')\n","\n","cluster0 = PCs_3d[targets == 0]\n","cluster1 = PCs_3d[targets == 1]\n","cluster2 = PCs_3d[targets == 2]\n","cluster3 = PCs_3d[targets == 3]\n","cluster4 = PCs_3d[targets == 4]\n","\n","\n","clusters = {k : np.stack([PCs_3d.loc[i].values for i in v]) for (k, v) in valset.group_partition.items()}\n","\n","traces = []\n","\n","for (k, v) in sorted(clusters.items()):\n","  traces.append(go.Scatter3d(\n","                    x = v[:, 0],\n","                    y = v[:, 1],\n","                    z = v[:, 2],\n","                    mode = \"markers\",\n","                    name = \"True label {}, Subgroup {}\".format(k[0], k[1]),\n","                    text = None))\n","\n","data = traces\n","\n","title = \"Visualizing Clusters in Three Dimensions Using PCA\"\n","\n","layout = dict(title = title,\n","              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n","              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n","             )\n","\n","fig = dict(data = data, layout = layout)\n","\n","iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPCEF2qQ5g-B"},"outputs":[],"source":["go.Figure.write_html(fig,f\"/content/drive/MyDrive/CS260D_final_project/plots/simCLR_{Config.spurious_correlation_strength}_SMALL_SSL_PCA3D_plot.html\") # write as html or image"]},{"cell_type":"markdown","metadata":{"id":"KgWi43BQ9B57"},"source":["## t-SNE 3D"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKWSytFx5hGi"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","perplexity = 50\n","\n","#T-SNE with two dimensions\n","tsne_3d = TSNE(n_components=3, perplexity=perplexity)\n","\n","TCs_3d = pd.DataFrame(tsne_3d.fit_transform(embeddings))\n","TCs_3d.columns = [\"TC1_3d\",\"TC3_2d\",\"TC3_3d\"]\n","tsne_embs = pd.concat([pd.DataFrame(embeddings),TCs_2d], axis=1, join='inner')\n","\n","tsne_3d_cluster0 = TCs_3d[targets == 0]\n","tsne_3d_cluster1 = TCs_3d[targets == 1]\n","tsne_3d_cluster2 = TCs_3d[targets == 2]\n","tsne_3d_cluster3 = TCs_3d[targets == 3]\n","tsne_3d_cluster4 = TCs_3d[targets == 4]\n","\n","\n","tsne_clusters = {k : np.stack([TCs_3d.loc[i].values for i in v]) for (k, v) in valset.group_partition.items()}\n","\n","tsne_3d_traces = []\n","\n","for (k, v) in sorted(tsne_clusters.items()):\n","  tsne_3d_traces.append(go.Scatter3d(\n","                    x = v[:, 0],\n","                    y = v[:, 1],\n","                    z = v[:, 2],\n","                    mode = \"markers\",\n","                    name = \"True label {}, Subgroup {}\".format(k[0], k[1]),\n","                    text = None))\n","\n","data = tsne_3d_traces\n","\n","title = \"Visualizing Clusters in Two Dimensions Using t-SNE\"\n","\n","layout = dict(title = title,\n","              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n","              yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False)\n","             )\n","\n","fig = dict(data = data, layout = layout)\n","\n","iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pe9Gw8Dd9gwJ"},"outputs":[],"source":["go.Figure.write_html(fig,f\"/content/drive/MyDrive/CS260D_final_project/plots/simCLR_{Config.spurious_correlation_strength}_SMALL_SSL_tSNE3D_plot.html\") # write as html or image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WusX2fbBJkXL"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1WaugWbAQ3h8IMSl9zTddJlBzTq8KIo8P","timestamp":1702408568350},{"file_id":"1MhjSG14mJPZt1jS_M9cGrltseRlzxdz4","timestamp":1702301881876},{"file_id":"1bpU9X-b2oGxwD8M-Zmihu0Ekv-tBswc9","timestamp":1702301742731},{"file_id":"1pb4WQPnlyoq75-4HlFqsFr0O-YsbPTYE","timestamp":1702236006923},{"file_id":"1UVbQ6EJzKVd6iEwXWJMjCEAKpuqlSpEz","timestamp":1702173439656},{"file_id":"1mEgwIsOPPYf7zT9UbXsMufkY6yxOZOe6","timestamp":1702172187045}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}